{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# house price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm ,rankdata\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import normaltest\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"housing_train.csv\")\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Shape: \"+str(df.shape))\n",
    "print()\n",
    "print(\"Columns: \"+str(df.columns))\n",
    "print()\n",
    "print(df.info())\n",
    "print()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My plan for data preparation: \n",
    " - drop duplicated rows if any.\n",
    " - remove outliers\n",
    " - filling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### price and sqfeet outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_outlier_removal(data, feature, min_q, max_q):\n",
    "    feature_min_outlier_mask = data[feature] > data[feature].quantile(min_q)\n",
    "    feature_max_outlier_mask = data[feature] < data[feature].quantile(max_q)\n",
    "    data = data[(feature_min_outlier_mask) & (feature_max_outlier_mask)]\n",
    "    print(feature, \"min: \", min(data[feature]))\n",
    "    print(feature, \"max: \", max(data[feature]))\n",
    "    return data\n",
    "\n",
    "def numerical_outlier_removal(data):\n",
    "    data = feature_outlier_removal(data, \"price\", 0.01, 0.999)\n",
    "    data = feature_outlier_removal(data, \"sqfeet\", 0.002, 0.999)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = numerical_outlier_removal(df)\n",
    "print(\"\\nOutliers Removed :\", df.shape[0] - raw_df.shape[0])\n",
    "print(\"Data Shape: \", raw_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### beds and baths outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = raw_df[raw_df['beds'] <= 6] \n",
    "raw_df = raw_df[raw_df['baths'] <= 3.5] \n",
    "\n",
    "print(\"Data Shape: \", raw_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lat and long outlier removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The valid range of latitude in degrees is -90 and +90 for the southern and northern hemisphere respectively. Longitude is in the range -180 and +180 specifying coordinates west and east of the Prime Meridian, respectively. But here we are dealing wiht US data, so, Lat-long coorditates for cities in United States are in range: Latitude from 19.50139 to 64.85694 and longitude from -161.75583 to -68.01197."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_min_mask = raw_df['lat'] >= 19.50139\n",
    "lat_max_mask = raw_df['lat'] <= 64.85694\n",
    "raw_df = raw_df[(lat_min_mask) & (lat_max_mask)]\n",
    "\n",
    "long_min_mask = raw_df['long'] >= -161.75583\n",
    "long_max_mask = raw_df['long'] <= -68.01197\n",
    "raw_df = raw_df[(long_min_mask) & (long_max_mask)]\n",
    "\n",
    "print(\"lat min: \", min(raw_df.lat))\n",
    "print(\"lat max: \", max(raw_df.lat))\n",
    "print(\"long min: \", min(raw_df.long))\n",
    "print(\"long max: \", max(raw_df.long))\n",
    "print(\"Data Shape: \", raw_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lat_long_outlier_removal(data):\n",
    "    data = feature_outlier_removal(data, \"lat\", 0.01, 0.999)\n",
    "    data = feature_outlier_removal(data, \"long\", 0.01, 0.999)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long_df = Lat_long_outlier_removal(raw_df)\n",
    "print(\"\\nOutliers Removed :\", raw_df.shape[0] - lat_long_df.shape[0])\n",
    "print(\"Data Shape: \", lat_long_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = raw_df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "print(missing)\n",
    "missing.sort_values(inplace=True)\n",
    "try:\n",
    "    missing.plot.bar()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### work on Laundry Options: Model based imputation (filling missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before imputation\n",
    "print(raw_df[\"laundry_options\"].value_counts())\n",
    "print(raw_df[\"laundry_options\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[\"laundry_options_na\"] = 0\n",
    "raw_df[\"laundry_options_na\"][raw_df[\"laundry_options\"][raw_df[\"laundry_options\"].isna()==True].index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "decide_cols = [\"beds\", \"baths\", \"cats_allowed\", \"dogs_allowed\", \n",
    "               \"smoking_allowed\", \"wheelchair_access\", \"electric_vehicle_charge\",\n",
    "               \"comes_furnished\", \"price\" ]\n",
    "\n",
    "X_train = raw_df[decide_cols][raw_df[\"laundry_options\"].isna()==False]\n",
    "y_train = raw_df[\"laundry_options\"][raw_df[\"laundry_options\"].isna()==False]\n",
    "X_test = raw_df[decide_cols][raw_df[\"laundry_options\"].isna()==True]\n",
    " \n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(X_train, y_train)\n",
    "laundry_pred = neigh.predict(X_test)\n",
    "print(laundry_pred)\n",
    "print(laundry_pred.size)\n",
    "\n",
    "# filling missing values\n",
    "raw_df[\"laundry_options\"][raw_df[\"laundry_options\"].isna()==True] = laundry_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after imputation\n",
    "print(raw_df[\"laundry_options\"].value_counts())\n",
    "print(raw_df[\"laundry_options\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### work on Parking Options: Model based imputation (filling missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before imputation\n",
    "print(raw_df[\"parking_options\"].value_counts())\n",
    "print(raw_df[\"parking_options\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[\"parking_options_na\"] = 0\n",
    "raw_df[\"parking_options_na\"][raw_df[\"parking_options\"][raw_df[\"parking_options\"].isna()==True].index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "decide_cols = [\"beds\", \"baths\", \"cats_allowed\", \"dogs_allowed\", \n",
    "               \"smoking_allowed\", \"wheelchair_access\", \"electric_vehicle_charge\",\n",
    "               \"comes_furnished\", \"price\" ]\n",
    "\n",
    "X_train = raw_df[decide_cols][raw_df[\"parking_options\"].isna()==False]\n",
    "y_train = raw_df[\"parking_options\"][raw_df[\"parking_options\"].isna()==False]\n",
    "X_test = raw_df[decide_cols][raw_df[\"parking_options\"].isna()==True]\n",
    " \n",
    "neigh = KNeighborsClassifier(n_neighbors=7)\n",
    "neigh.fit(X_train, y_train)\n",
    "laundry_pred = neigh.predict(X_test)\n",
    "print(laundry_pred)\n",
    "print(laundry_pred.size)\n",
    "\n",
    "# filling missing values\n",
    "\n",
    "raw_df[\"parking_options\"][raw_df[\"parking_options\"].isna()==True] = laundry_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after imputation\n",
    "print(raw_df[\"parking_options\"].value_counts())\n",
    "print(raw_df[\"parking_options\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### removing remaining empty features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_df.isnull().sum())\n",
    "raw_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = raw_df.copy()\n",
    "print(clean_df.columns)\n",
    "print(clean_df.info())\n",
    "clean_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My plan for data preparation:\n",
    " - firstly remove unnecessary cols\n",
    " - create sqfeet range column\n",
    " - work on url\n",
    " - work on Latitude and Longitude\n",
    " - work on description\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    clean_df = clean_df.drop(['url', 'region_url', 'image_url'], axis=1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Create sqfeet range column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://www.point2homes.com/news/wp-content/uploads/2017/01/Home-Size-Table-by-Province-CA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sqfeet_range_column(data, feature='sqfeet'):\n",
    "#     if data[feature] < 300:\n",
    "#         return 'single room'\n",
    "#     if data[feature] >= 300 and data[feature] < 500:\n",
    "#         return 'mini'\n",
    "#     if data[feature] >= 500 and data[feature] < 1000:\n",
    "#         return 'small'\n",
    "#     if data[feature] >= 1000 and data[feature] < 1500:\n",
    "#         return 'medium'\n",
    "#     if data[feature] >= 1500 and data[feature] < 2000:\n",
    "#         return 'large'\n",
    "#     if data[feature] >= 2000 and data[feature] < 2500:\n",
    "#         return 'extra large'\n",
    "#     if data[feature] >=2500:\n",
    "#         return 'mansion'\n",
    "    \n",
    "\n",
    "# clean_df['sqfeet_range'] = clean_df.apply(sqfeet_range_column, axis=1)\n",
    "# clean_df.sqfeet_range.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. work on url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO: open this\n",
    "# from urllib.parse import urlparse, parse_qs\n",
    "# from tqdm import tqdm\n",
    "# disc = {\"url_parsed_loc\": [], \"url_params\":[], \"url_path_components\": []}\n",
    "\n",
    "# for i in tqdm(range(df.shape[0]), position=0, leave=True):\n",
    "#     parsed = urlparse(df.url[i])\n",
    "#     disc[\"url_parsed_loc\"].append(parsed.netloc)\n",
    "    \n",
    "#     params = parse_qs(parsed.query)\n",
    "#     disc[\"url_params\"].append(params)\n",
    "    \n",
    "#     path_components = list(filter(bool, parsed.path.split('/')))\n",
    "#     disc[\"url_path_components\"].append(path_components)\n",
    "\n",
    "\n",
    "# #TODO: open this\n",
    "\n",
    "# url_df = pd.DataFrame(disc)\n",
    "# print(url_df.url_params.value_counts())\n",
    "# url_df.head()\n",
    "\n",
    "# #TODO: open this\n",
    "# url_df.drop(\"url_params\", axis=1, inplace=True)\n",
    "# url_df.head()\n",
    "\n",
    "\n",
    "\n",
    "# #TODO: open this\n",
    "# cnt=0\n",
    "# for i in tqdm(range(df.shape[0]), position=0, leave=True):\n",
    "#     if df[\"region_url\"][i].find(url_df[\"url_parsed_loc\"][i]) >= 0: \n",
    "#         cnt+=1\n",
    "\n",
    "# print(\"Count: \"+str(cnt))\n",
    "# print(\"Error rate: \"+str(1-(cnt/df.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. work on Latitude and Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: open this\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# sse={}\n",
    "# lat_long_df = clean_df[['lat', 'long']]\n",
    "\n",
    "# for k in tqdm(range(1, 12), position=0, leave=True):\n",
    "#     kmeans = KMeans(n_clusters=k, max_iter=1000).fit(lat_long_df)\n",
    "#     lat_long_df[\"clusters\"] = kmeans.labels_\n",
    "#     sse[k] = kmeans.inertia_ \n",
    "# plt.figure()\n",
    "# plt.plot(list(sse.keys()), list(sse.values()))\n",
    "# plt.xlabel(\"Number of cluster\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258418\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=8, random_state=0)\n",
    "lat_long_pred = kmeans.fit_predict(clean_df[[\"lat\", \"long\"]])\n",
    "print(lat_long_pred.size)\n",
    "clean_df['lat_long_cluster'] = lat_long_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(figsize=(10,10))\n",
    "plt.scatter(x=clean_df['lat'], y=clean_df['long'], c=lat_long_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.work on description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "#             \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "#             'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "#             'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "#             'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "#             'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "#             'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "#             'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "#             'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "#             'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "#             's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "#             've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "#             \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "#             \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "#             'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decontracted(phrase):\n",
    "#     # specific\n",
    "#     phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "#     phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "#     # general\n",
    "#     phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "#     phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "#     phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "#     phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "#     phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "#     phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "#     phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "#     phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "#     return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(sentance):\n",
    "#     sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "#     sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "#     sentance = decontracted(sentance)\n",
    "#     sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "#     sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "#     # https://gist.github.com/sebleier/554280\n",
    "#     sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "#     return sentance.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(loop), position=0, leave=True):\n",
    "#     df.description[i] = clean_text(df.description[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()\n",
    "# df = df.drop(df.index[4]).reset_index()\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(loop), position=0, leave=True):\n",
    "#     try:\n",
    "#         df.description[i] = clean_text(df.description[i])\n",
    "#     except:\n",
    "#         df.drop(df.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "# sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "# description_dict = {\"description_negative\":[], \"description_neutral\": [], \"description_positive\":[]}\n",
    "\n",
    "\n",
    "# loop = clean_df.shape[0]\n",
    "# for i in tqdm(range(loop), position=0, leave=True):\n",
    "#     desc = str(clean_df.description[i])\n",
    "#     sentiment_dict = sid_obj.polarity_scores(desc) \n",
    "#     description_dict[\"description_negative\"].append(sentiment_dict[\"neg\"])\n",
    "#     description_dict[\"description_neutral\"].append(sentiment_dict[\"neu\"])\n",
    "#     description_dict[\"description_positive\"].append(sentiment_dict[\"pos\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(description_dict, './pickles/description_dict.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description_dict = joblib.load('./pickles/description_dict.pkl') \n",
    "\n",
    "# desc_df = pd.DataFrame(description_dict)\n",
    "# print(desc_df.shape)\n",
    "# desc_df.head()\n",
    "\n",
    "# clean_df = pd.concat([clean_df, desc_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.drop([\"description\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.corr()\n",
    "f, ax = plt.subplots(figsize=(16, 16))\n",
    "sns.heatmap(clean_df.corr(), annot=True, linewidths=0.5, square=True, vmax=0.3, center=0, cmap=sns.cubehelix_palette())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('clean_df.csv', index = False)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clean_df.csv\")\n",
    "# df.drop(['state'], axis=1, inplace=True)\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258418, 21)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df,drop_first=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.iloc[:1000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df.drop([\"id\", \"price\"], axis=1)\n",
    "df_y = df.loc[:, \"price\"]\n",
    "print(df_X.info())\n",
    "df_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_X = scaler.fit_transform(df_X)\n",
    "# X_test = scaler.transform(X_test)\n",
    "print(df_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# regressor = Sequential()\n",
    "# # Adding the first LSTM layer and some Dropout regularisation\n",
    "# regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "# regressor.add(Dropout(0.2))\n",
    "# regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "# regressor.add(Dropout(0.2))\n",
    "# regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "# regressor.add(Dropout(0.2))\n",
    "# regressor.add(LSTM(units = 50))\n",
    "# regressor.add(Dropout(0.2))\n",
    "\n",
    "# regressor.add(Dense(units = 1))\n",
    "\n",
    "# regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "# regressor.fit(X_train, y_train, epochs = 1, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "def calculate_regression_metrics(y_test, predictions):\n",
    "    mean_squared_error = skmetrics.mean_squared_error(y_test, predictions)\n",
    "    mean_absolute_error = skmetrics.mean_absolute_error(y_test, predictions)\n",
    "    r2_error = skmetrics.r2_score(y_test, predictions)\n",
    "    result = {'mean_squared_error': mean_squared_error, 'mean_absolute_error': mean_absolute_error, 'r2_score': r2_error}\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn import linear_model\n",
    "\n",
    "# poly = PolynomialFeatures(degree=2)\n",
    "# X_train_ = poly.fit_transform(X_train)\n",
    "# X_test_ = poly.fit_transform(X_test)\n",
    "\n",
    "# clf = linear_model.LinearRegression()\n",
    "# clf.fit(X_, y_train)\n",
    "# ppp = clf.predict(X_test_)\n",
    "# print(skmetrics.r2_score(y_test, ppp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = RandomForestRegressor(n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "pred = pred.reshape(-1, 1)\n",
    "\n",
    "print(pred)\n",
    "print(\"//////////////////////////////////////\")\n",
    "print(y_test)\n",
    "print(\"//////////////////////////////////////\")\n",
    "\n",
    "calculate_regression_metrics(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_test = {\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': range(20, 100, 20),\n",
    "#     'max_features': [2, 3],\n",
    "#     'min_samples_leaf': [2, 4, 6],\n",
    "#     'min_samples_split': [6, 10, 14],\n",
    "#     'n_estimators': range(50, 500, 100)\n",
    "# }\n",
    "\n",
    "# gsearch = GridSearchCV(estimator = RandomForestRegressor(n_jobs=-1), \n",
    "#                        param_grid = param_test, \n",
    "#                           cv = 2, \n",
    "#                        n_jobs = -1, \n",
    "#                        verbose = 2)\n",
    "\n",
    "\n",
    "# gsearch.fit(X_train,y_train)\n",
    "# print(gsearch.best_params_, gsearch.best_score_)\n",
    "\n",
    "# tuned_pred = gsearch.predict(X_test)\n",
    "# model_evaluation(y_test, tuned_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, figsize=(21,21))\n",
    "# ax.scatter(range(y_test.size), y_test)\n",
    "# ax.scatter(range(y_test.size), pred)\n",
    "\n",
    "# # ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "# ax.set_xlabel('Measured')\n",
    "# ax.set_ylabel('Predicted')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, figsize=(21,21))\n",
    "# ax.plot(y_test)\n",
    "# ax.plot(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xg_reg = xgb.XGBRegressor(n_jobs=6)\n",
    "# xg_reg.fit(X_train, y_train)\n",
    "# xg_reg = xg_reg.predict(X_test)\n",
    "\n",
    "\n",
    "# pred = xg_reg.reshape(-1, 1)\n",
    "\n",
    "# # pred = scalery.inverse_transform(pred)\n",
    "# # y_test = scalery.inverse_transform(y_test)\n",
    "\n",
    "# print(pred)\n",
    "# print(\"//////////////////////////////////////\")\n",
    "# print(y_test)\n",
    "# print(\"//////////////////////////////////////\")\n",
    "\n",
    "\n",
    "\n",
    "# calculate_regression_metrics(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-58-e58ba6b6f4c4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-58-e58ba6b6f4c4>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    testing below\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "testing below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 25 18:06:28 2020\n",
    "\n",
    "@author: paras\n",
    "\"\"\"\n",
    "\n",
    "from flask import Flask, request\n",
    "import flask\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return flask.render_template('index.html')\n",
    "\n",
    "\n",
    "def clean_data(raw_df):     \n",
    "    lat_min_mask = raw_df['lat'] >= 19.50139\n",
    "    lat_max_mask = raw_df['lat'] <= 64.85694\n",
    "    raw_df = raw_df[(lat_min_mask) & (lat_max_mask)]\n",
    "    long_min_mask = raw_df['long'] >= -161.75583\n",
    "    long_max_mask = raw_df['long'] <= -68.01197\n",
    "    raw_df = raw_df[(long_min_mask) & (long_max_mask)]    \n",
    "    raw_df.dropna(inplace=True)\n",
    "    return raw_df\n",
    "\n",
    "\n",
    "def feature_engineer_data(clean_df):\n",
    "    print(\"test0: ###########################\")\n",
    "    print(clean_df.shape)\n",
    "    try:\n",
    "        clean_df = clean_df.drop(['id', 'url', 'region_url', 'image_url', 'description'], axis=1)\n",
    "    except:\n",
    "        print(\"Custom Error: drop columns did not execute!!\")\n",
    "    \n",
    "    lat_long_pred = lat_long_classifier.predict(clean_df[[\"lat\", \"long\"]])\n",
    "    print(\"test1: ###########################\")\n",
    "    print(clean_df.shape)\n",
    "    clean_df['lat_long_cluster'] = lat_long_pred\n",
    "    clean_df = clean_df.reset_index(drop=True)\n",
    "    clean_df = clean_df.reindex(sorted(clean_df.columns), axis=1)\n",
    "    clean_df.fillna(-1)\n",
    "    \n",
    "    print(\"test2: ###########################\")\n",
    "    print(clean_df.shape)\n",
    "    \n",
    "    clean_df = pd.get_dummies(clean_df,drop_first=True)\n",
    "    print(\"test3: ###########################\")\n",
    "    print(clean_df.shape)\n",
    "    return clean_df\n",
    "\n",
    "def scale_data(df):\n",
    "    new_df = min_max_scaler.transform(df)\n",
    "    return new_df\n",
    "\n",
    "def prdict_results(df):\n",
    "    random_regressor_pred = random_regressor.predict(df)\n",
    "    return random_regressor_pred\n",
    "\n",
    "\n",
    "\n",
    "def process_input_data(df_input):\n",
    "    int_cols = ['id', 'sqfeet', 'beds', 'cats_allowed', \n",
    "                'dogs_allowed', 'smoking_allowed', \n",
    "                'wheelchair_access', 'electric_vehicle_charge', \n",
    "                'comes_furnished']\n",
    "    float_cols = ['baths', 'lat', 'long']\n",
    "    \n",
    "    df_input[int_cols] = df_input[int_cols].astype('int64')\n",
    "    df_input[float_cols] = df_input[float_cols].astype('float64')\n",
    "    print(df_input.info())\n",
    "    \n",
    "    print('CLEANING DATA..............')\n",
    "    clean_df = clean_data(df_input)\n",
    "    print('FEATURING DATA.............')\n",
    "    df_featured = feature_engineer_data(clean_df)  \n",
    "    print(\"test4: ###########################\")\n",
    "    print(df_featured.shape)\n",
    "    print(\"DATA COLUMNS: //////////////\")\n",
    "    print(data_columns)\n",
    "    sample_df = pd.DataFrame(columns = data_columns)\n",
    "    main_df = sample_df.append(df_featured)\n",
    "    main_df = main_df.fillna(0)\n",
    "    print(\"MAIN DATAFRAME: //////////////\")\n",
    "    print(main_df)\n",
    "    print(main_df.info())\n",
    "    print(main_df.columns)\n",
    "    \n",
    "    for i in main_df.columns:\n",
    "        if main_df[i].dtypes == 'float64':\n",
    "            print(i, end=\"\\n\\n\")\n",
    "    \n",
    "    print('SCALING DATA.............')\n",
    "    df_scaled = scale_data(main_df)\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    form_data = request.form.to_dict()\n",
    "    print(\"FORM DATA: //////////////\")\n",
    "    print(form_data)\n",
    "    \n",
    "    df_input = pd.DataFrame.from_records([form_data])\n",
    "    df_input = pd.DataFrame(df_input)\n",
    "    print(\"INPUT DATAFRAME: //////////////\")\n",
    "    print(df_input)       \n",
    "    \n",
    "    df_scaled = process_input_data(df_input)\n",
    "    \n",
    "    pred_val = \"\"\n",
    "    pred_val = np.round(prdict_results(df_scaled), 2)\n",
    "    print(\"PREDICTION: ////////////////\")\n",
    "    print(pred_val)\n",
    "    msg = f\"Wohoo! AI predicts the price of this property to be around {pred_val[0]} $\"\n",
    "    return flask.render_template('index.html', \n",
    "                                 predicted_value=\"{}\".format(\"Prediction: \"+str(pred_val[0])+\" $\"), \n",
    "                                 any_message=msg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/predict_multiple', methods=['POST'])\n",
    "def predict_multiple():\n",
    "    form_data = request.form.to_dict()\n",
    "    print(\"FORM DATA\")\n",
    "    form_data_array = np.array(form_data[\"myarray\"])\n",
    "    print(form_data_array)\n",
    "\n",
    "    js_df = pd.read_json(form_data[\"myarray\"])\n",
    "    \n",
    "    df_input = pd.DataFrame.from_records(js_df)\n",
    "    \n",
    "    df_input.columns = df_input.iloc[0]\n",
    "    df_input = df_input.iloc[1:, 1:]\n",
    "    print(\"INPUT DATAFRAME\")\n",
    "    print(df_input.head())\n",
    "    print(df_input.info())\n",
    "    \n",
    "    df_scaled = process_input_data(df_input)\n",
    "    \n",
    "\n",
    "    pred_val = \"\"\n",
    "    msg = \"Wohoo! AI predicts the price of this property.\"\n",
    "\n",
    "    pred_val = prdict_results(df_scaled)\n",
    "    print(\"PREDICTION: ////////////////\")\n",
    "    print(pred_val)\n",
    "    \n",
    "    res = pd.DataFrame({\"id\": df_input[\"id\"], \"prediction\": pred_val})\n",
    "    print(\"RESULT: //////////////\")\n",
    "    print(res) \n",
    "    res_json = res.to_json(orient='records')\n",
    "    return flask.render_template('index.html', \n",
    "                                 predicted_value_multi=str(res_json), \n",
    "                                 any_message_multi=msg)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    random_regressor = joblib.load(\"./pickles/random_regressor.pkl\")\n",
    "    min_max_scaler = joblib.load(\"./pickles/min_max_scaler.pkl\")\n",
    "    data_columns = joblib.load(\"./pickles/data_columns.pkl\")\n",
    "    lat_long_classifier = joblib.load(\"./pickles/lat_long_classifier.pkl\")\n",
    "    \n",
    "    app.run(host='0.0.0.0', port=8088)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
